"""
Model Validator Module

Provides model validation and diagnostic functions for video classification.
"""

import os
import json
import numpy as np
from pathlib import Path
from collections import defaultdict, Counter
import argparse


class ModelValidator:
    """Classe para valida√ß√£o e diagn√≥stico de modelos"""
    
    def __init__(self):
        self.results = {}
    
    def validate_model_structure(self, model_path):
        """
        Valida a estrutura b√°sica do modelo
        
        Args:
            model_path: Caminho do modelo
            
        Returns:
            dict: Resultado da valida√ß√£o
        """
        print(f"üîç Validando estrutura do modelo: {model_path}")
        
        issues = []
        info = {}
        
        # Verificar se o diret√≥rio existe
        if not os.path.exists(model_path):
            issues.append(f"Diret√≥rio do modelo n√£o existe: {model_path}")
            return {'valid': False, 'issues': issues, 'info': info}
        
        # Verificar arquivo do classificador
        classifier_path = os.path.join(model_path, "classifier.pkl")
        if not os.path.exists(classifier_path):
            issues.append("Arquivo classifier.pkl n√£o encontrado")
        else:
            info['classifier_file'] = classifier_path
            
            # Verificar tamanho do arquivo
            size_mb = os.path.getsize(classifier_path) / (1024 * 1024)
            info['classifier_size_mb'] = round(size_mb, 2)
            
            if size_mb < 0.1:
                issues.append("Arquivo classifier.pkl muito pequeno (pode estar corrompido)")
            elif size_mb > 100:
                issues.append("Arquivo classifier.pkl muito grande (pode haver problema)")
        
        # Verificar arquivo de configura√ß√£o
        config_path = os.path.join(model_path, "config.json")
        if not os.path.exists(config_path):
            issues.append("Arquivo config.json n√£o encontrado")
        else:
            info['config_file'] = config_path
            
            try:
                with open(config_path, 'r') as f:
                    config = json.load(f)
                    info['classes'] = config.get('classes', [])
                    info['model_type'] = config.get('model_type', 'Unknown')
                    
                    if not info['classes']:
                        issues.append("Classes n√£o definidas na configura√ß√£o")
                        
            except json.JSONDecodeError:
                issues.append("Arquivo config.json corrompido")
            except Exception as e:
                issues.append(f"Erro ao ler config.json: {e}")
        
        # Verificar outros arquivos opcionais
        optional_files = ['hybrid_config.json', 'training_log.txt']
        for file in optional_files:
            file_path = os.path.join(model_path, file)
            if os.path.exists(file_path):
                info[f'has_{file.replace(".", "_")}'] = True
        
        # Resultado final
        is_valid = len(issues) == 0
        
        if is_valid:
            print("‚úÖ Estrutura do modelo v√°lida")
        else:
            print("‚ùå Problemas encontrados na estrutura:")
            for issue in issues:
                print(f"  ‚Ä¢ {issue}")
        
        return {
            'valid': is_valid,
            'issues': issues,
            'info': info
        }
    
    def analyze_model_performance(self, model_path, dataset_path, max_videos_per_class=10):
        """
        Analisa performance detalhada do modelo
        
        Args:
            model_path: Caminho do modelo
            dataset_path: Caminho do dataset de teste
            max_videos_per_class: M√°ximo de v√≠deos por classe para teste
            
        Returns:
            dict: Resultados da an√°lise
        """
        print(f"\nüîç === AN√ÅLISE DE PERFORMANCE ===")
        print(f"üìÅ Modelo: {model_path}")
        print(f"üìÇ Dataset: {dataset_path}")
        
        # Primeiro validar estrutura
        structure_check = self.validate_model_structure(model_path)
        if not structure_check['valid']:
            return {
                'success': False,
                'error': 'Estrutura do modelo inv√°lida',
                'structure_issues': structure_check['issues']
            }
        
        try:
            # Carregar modelo usando VideoModelTrainer
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.abspath(__file__)))
            from model_trainer import VideoModelTrainer
            
            trainer = VideoModelTrainer()
            
            # Carregar modelo
            if not trainer.load_model(model_path):
                return {
                    'success': False,
                    'error': 'Falha ao carregar o modelo'
                }
            
            # Avaliar no dataset
            print(f"üéØ Avaliando modelo no dataset...")
            results = trainer.evaluate_on_dataset(dataset_path, max_videos_per_class)
            
            if not results.get('success'):
                return {
                    'success': False,
                    'error': results.get('error', 'Erro na avalia√ß√£o')
                }
            
            # Formatar resultados
            performance_results = {
                'success': True,
                'model_info': structure_check['info'],
                'performance': {
                    'accuracy': results['accuracy'],
                    'total_videos': results['total_videos'],
                    'classes_tested': results['classes_tested'],
                    'predictions': results['predictions'],
                    'classification_report': results['classification_report']
                },
                'recommendations': []
            }
            
            # Gerar recomenda√ß√µes
            accuracy = results['accuracy']
            if accuracy < 0.7:
                performance_results['recommendations'].append("Acur√°cia baixa - considere mais dados de treino ou ajuste de hiperpar√¢metros")
            elif accuracy > 0.95:
                performance_results['recommendations'].append("Poss√≠vel overfitting - validar com dataset independente")
            
            if results['total_videos'] < 20:
                performance_results['recommendations'].append("Dataset de teste pequeno - considere mais amostras para valida√ß√£o confi√°vel")
            
            print(f"\nüìä Resultados:")
            print(f"   üéØ Acur√°cia: {accuracy:.1%}")
            print(f"   üìà V√≠deos testados: {results['total_videos']}")
            print(f"   üìã Classes: {', '.join(results['classes_tested'])}")
            
            return performance_results
            
        except Exception as e:
            print(f"‚ùå Erro na an√°lise: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def compare_models(self, model1_path, model2_path, dataset_path, max_videos_per_class=10):
        """
        Compara performance entre dois modelos
        
        Args:
            model1_path: Caminho do primeiro modelo
            model2_path: Caminho do segundo modelo
            dataset_path: Caminho do dataset de teste
            max_videos_per_class: M√°ximo de v√≠deos por classe para teste
            
        Returns:
            dict: Compara√ß√£o entre os modelos
        """
        print(f"\nüÜö === COMPARA√á√ÉO DE MODELOS ===")
        
        # Analisar modelo 1
        print(f"\nüìä Analisando Modelo 1...")
        results1 = self.analyze_model_performance(model1_path, dataset_path, max_videos_per_class)
        
        # Analisar modelo 2  
        print(f"\nüìä Analisando Modelo 2...")
        results2 = self.analyze_model_performance(model2_path, dataset_path, max_videos_per_class)
        
        # Comparar resultados
        comparison = {
            'model1': {
                'path': model1_path,
                'results': results1
            },
            'model2': {
                'path': model2_path, 
                'results': results2
            },
            'winner': None,
            'differences': []
        }
        
        # Determinar vencedor (se ambos tiveram sucesso)
        if results1.get('success') and results2.get('success'):
            acc1 = results1.get('performance', {}).get('accuracy', 0)
            acc2 = results2.get('performance', {}).get('accuracy', 0)
            
            if acc1 > acc2:
                comparison['winner'] = 'model1'
                comparison['differences'].append(f"Modelo 1 tem melhor acur√°cia: {acc1:.4f} vs {acc2:.4f}")
            elif acc2 > acc1:
                comparison['winner'] = 'model2'
                comparison['differences'].append(f"Modelo 2 tem melhor acur√°cia: {acc2:.4f} vs {acc1:.4f}")
            else:
                comparison['winner'] = 'tie'
                comparison['differences'].append(f"Ambos t√™m acur√°cia similar: {acc1:.4f}")
        
        return comparison
    
    def generate_diagnosis_report(self, model_path, dataset_path=None):
        """
        Gera relat√≥rio completo de diagn√≥stico
        
        Args:
            model_path: Caminho do modelo
            dataset_path: Caminho do dataset (opcional)
            
        Returns:
            dict: Relat√≥rio completo
        """
        print(f"\nüìã === RELAT√ìRIO DE DIAGN√ìSTICO ===")
        
        report = {
            'model_path': model_path,
            'timestamp': None,  # Seria implementado
            'structure_check': self.validate_model_structure(model_path),
            'performance_analysis': None,
            'recommendations': []
        }
        
        # An√°lise de performance se dataset fornecido
        if dataset_path:
            report['performance_analysis'] = self.analyze_model_performance(model_path, dataset_path)
        
        # Gerar recomenda√ß√µes
        if not report['structure_check']['valid']:
            report['recommendations'].append("Corrigir problemas na estrutura do modelo")
        
        if dataset_path and report['performance_analysis']:
            if not report['performance_analysis'].get('success'):
                report['recommendations'].append("Investigar problemas na an√°lise de performance")
        
        return report


def list_available_models(models_dir="models"):
    """
    Lista todos os modelos dispon√≠veis
    
    Args:
        models_dir: Diret√≥rio dos modelos
        
    Returns:
        list: Lista de modelos encontrados
    """
    if not os.path.exists(models_dir):
        print(f"üìÇ Diret√≥rio de modelos n√£o encontrado: {models_dir}")
        return []
    
    models = []
    for item in os.listdir(models_dir):
        model_path = os.path.join(models_dir, item)
        if os.path.isdir(model_path):
            # Verificar se tem arquivos de modelo
            classifier_file = os.path.join(model_path, "classifier.pkl")
            if os.path.exists(classifier_file):
                models.append({
                    'name': item,
                    'path': model_path,
                    'has_config': os.path.exists(os.path.join(model_path, "config.json"))
                })
    
    return models


if __name__ == "__main__":
    """Interface de linha de comando para valida√ß√£o de modelos"""
    parser = argparse.ArgumentParser(description="Validador de Modelos para Classifica√ß√£o de V√≠deos")
    parser.add_argument('command', choices=['validate', 'analyze', 'compare', 'list'], 
                       help='Comando a executar')
    parser.add_argument('--model', 
                       help='Caminho do modelo')
    parser.add_argument('--model2',
                       help='Caminho do segundo modelo (para compara√ß√£o)')
    parser.add_argument('--dataset',
                       help='Caminho do dataset de teste')
    parser.add_argument('--models-dir', default='models',
                       help='Diret√≥rio dos modelos')
    parser.add_argument('--max-videos', type=int, default=10,
                       help='M√°ximo de v√≠deos por classe')
    
    args = parser.parse_args()
    
    validator = ModelValidator()
    
    if args.command == 'list':
        print("üìã Listando modelos dispon√≠veis...")
        models = list_available_models(args.models_dir)
        
        if not models:
            print("‚ùå Nenhum modelo encontrado")
        else:
            print(f"‚úÖ Encontrados {len(models)} modelos:")
            for model in models:
                status = "‚úÖ Completo" if model['has_config'] else "‚ö†Ô∏è Sem config"
                print(f"  ü§ñ {model['name']}: {status}")
    
    elif args.command == 'validate':
        if not args.model:
            print("‚ùå --model √© obrigat√≥rio para valida√ß√£o")
            exit(1)
        
        print("üîç Validando modelo...")
        result = validator.validate_model_structure(args.model)
        
        if result['valid']:
            print("‚úÖ Modelo v√°lido!")
        else:
            print("‚ùå Modelo inv√°lido!")
            exit(1)
    
    elif args.command == 'analyze':
        if not args.model:
            print("‚ùå --model √© obrigat√≥rio para an√°lise")
            exit(1)
            
        if not args.dataset:
            print("‚ùå --dataset √© obrigat√≥rio para an√°lise")
            exit(1)
        
        print("üìä Analisando performance...")
        result = validator.analyze_model_performance(args.model, args.dataset, args.max_videos)
        
        if result.get('success'):
            print("‚úÖ An√°lise conclu√≠da!")
        else:
            print(f"‚ùå Falha na an√°lise: {result.get('error', 'Erro desconhecido')}")
            exit(1)
    
    elif args.command == 'compare':
        if not args.model or not args.model2:
            print("‚ùå --model e --model2 s√£o obrigat√≥rios para compara√ß√£o")
            exit(1)
            
        if not args.dataset:
            print("‚ùå --dataset √© obrigat√≥rio para compara√ß√£o")
            exit(1)
        
        print("üÜö Comparando modelos...")
        result = validator.compare_models(args.model, args.model2, args.dataset, args.max_videos)
        
        winner = result.get('winner')
        if winner:
            print(f"üèÜ Vencedor: {winner}")
        else:
            print("‚ùå N√£o foi poss√≠vel determinar vencedor")